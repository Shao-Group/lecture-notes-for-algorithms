\setcounter{definition}{0} \setcounter{property}{0} \setcounter{claim}{0} \setcounter{fact}{0} \setcounter{corollary}{0} \setcounter{figure}{0}
\section{Polynomial-Time Reduction}

In Lecture~39, we designed an algorithm to solve the maximum matching problem on bipartite graphs
by \emph{transforming} it into a maximum-flow problem. This algorithm calls once a
solver for the maximum-flow problem~(in step 2), and it also builds the network~(in step 1)
and fetch the matching by processing the maximum-flow~(in step 3). This is a typical
example of so-called \emph{polynomial-time reduction}.
Such {reduction} is extremely useful---not only in solving new problems by using existing algorithms,
but also in proving that a problem is hard. Below we first formally define it.

\begin{definition}[Polynomial-Time Reduction]
Let $X$ and $Y$ be two problems.
If there exists an algorithm for $X$ which
calls a polynomial number of an algorithm for $Y$
and uses polynomial number of extra computational steps,
then we say $X$ is polynomial-time reducible to $Y$, denoted as $X \le_p Y$.
\end{definition}

In short, problem $X$ is polynomial-time reducible to problem $Y$ means ``we can design an algorithm for $X$ using an algorithm for $Y$''.
Of course, in this algorithm for $X$ the number of calls of a solver for $Y$ is limited to polynomial, and this algorithm for $X$
can have extra procedures, but these again need to run in polynomial-time.

Our algorithm for solving the maximum matching problem on bipartite graphs is such an example:
in that algorithm, we called once a solver for the maximum-flow problem; and the steps 1 and 3
takes polynomial-time. Therefore, we have (maximum-matching-problem-on-bipartite-graph) $\le_p$ (maximum-flow-problem).

Note that, above definition of polynomial-time reduction does \emph{not} require that
the algorithm for $Y$ runs in polynomial-time. It does not matter whether or not $Y$ can
be solved in polynomial-time. But, \emph{if} the solver for $Y$ runs in polynomial-time,
then the resulting algorithm for $X$ runs in polynomial-time as well, because 
a polynomial number of calls of an algorithm runs in polynomial-time also takes polynomial-time.
Formally,
\begin{fact}
If $X\le_p Y$ and $Y$ can be solved in polynomial-time, then $X$ can be solved in polynomial-time.
\end{fact}

\subsection*{Decision Problems}

A \emph{decision problem} refers to a problem with a \emph{binary} output, i.e., an algorithm
for this problem answers ``yes'' or ``no''.
For example, the 3SAT problem, which asks if a set of given clauses can be satisfied, is a decision problem.
Many of the problems we have seen in this course are \emph{optimization problems}. For examples, the maximum-flow problems
asks a flow, which is a function that assigns a number to each edge, of a given network, such that its value is maximized;
the (minimum) vertex cover problem asks a vertex cover, which is a subset of vertices, of a given undirected graph, 
such that its cardinality is minimized. These optimization problems are not decision problems.

Usually we can define a \emph{decision-version} of an optimization problem. Let's try it for the vertex cover problem;
its decision-version can be defined as follows: given an undirected graph $G = (V, E)$ and an integer $k\ge 0$,
decide if there exists a vertex cover $V_1$ of $G$ such that $|V_1| \le k$. In other words, the problem asks if the
given $k$ is an upper-bound of the cardinality of the minimum vertex cover of the given undirected graph.
%Note that its input includes both an undirected graph and an integer.

We use $VC(G)$ to refer to the (optimization-version) of the vertex-cover problem, and use $VC(G, k)$ to refer to
above decision-version of the vertex-cover problem. What's the ralationship between them? Here by ``relationship''
we specifically ask if one problem is polynomial-time reducible to the other. Intuitively, the decision-version
is ``easier'' than the optimization-version, and we can easily use an algorithm for the optimization-version
to solver the decision-version.
\begin{fact}
$VC(G, k)\le_p VC(G)$.
\end{fact}
\emph{Proof.} To prove that $VC(G,k)$ is polynomial-time reducible to $VC(G)$, by definition, 
we need to design an algorithm for $VC(G, k)$, in which we can assume and use, up to polynomial number of calls,
a solver for $VC(G)$. Here is such an algorithm:

\begin{minipage}{0.8\textwidth}
	\aaA {4}{algorithm-for-$VC(G, k)$}\xxx
	\aab {call the solver for $VC(G)$ to get the minimum vertex cover of $G$: $V_1^* \leftarrow $ solver-for-$VC(G)$;}\xxx
	\aab {if $|V_1^*| \le k$: return ``yes'';}\xxx
	\aab {else: return ``no'';}\xxx
	\aaa {end;}\xxx
\end{minipage}

Clearly, this algorithm for $VC(G, k)$ is correct; it calls once (and hence polynomial number)
a solver for $VC(G)$, and the additional procedure~(which just compares $|V_1^*|$ with the given $k$)
runs in constant time~(and hence in polynomial-time). This proves that $VC(G, k)\le_p VC(G)$. \qed

How about the other side? Is $VC(G)$ polynomial-time reducible to $VC(G, k)$? It's not obvious but 
this is true as well. Formally,
\begin{fact}
$VC(G)\le_p VC(G, k)$.
\end{fact}
\emph{Proof.}  Again we need to design an algorithm for $VC(G)$ using a solver for $VC(G, k)$.
Recall that a solver for the decision-version tells if a given $k$ is an upper-bound of the
cardinality of the minimum vertex cover of $G$. 
Therefore we can calculate the \emph{cardinality} of the minimum vertex cover of $G$,
by testing all possible sizes of a subset of vertices in \emph{increasing} order,
   i.e., $0, 1, 2, \cdots, |V|$;
the first achievable value is the \emph{cardinality} of the minimum vertex cover.

\begin{minipage}{0.8\textwidth}
	\aaA {5}{algorithm-for-cardinality-$VC(G)$}\xxx
	\aaB {3}{for $k = 0 \to |V|$}\xxx
	\aac {call the solver for the decision-version to test the given $G$ and this $k$: $B \leftarrow $ solver-for-$VC(G, k)$;}\xxx
	\aac {if $B$ = ``yes'': return $k$;}\xxx
	\aab {end for;}\xxx
	\aaa {end;}\xxx
\end{minipage}

This algorithm finds the cardinality of the minimum vertex cover of $G$; we denote it by $k^*$.
How can we then find the actual vertex cover? Here is one strategy:
we determine a single vertex that must be in one minimum vertex cover; if
this can be done, we can then
include that vertex (to the minimum vertex cover) and resolve the ``remaining part'' of the graph recursively.
The core procedure is therefore to determine if a vertex is in one minimum vertex cover,
which can be solved based on this fact: let $v\in V$ be a vertex; 
there exists a minimum vertex cover that includes $v$
if and only if the ``remaining-graph'' $G_{-v}$, defined as the graph by removing $v$ and adjacent edges of $v$ from $G$, can
be covered by $(k^* - 1)$ vertices. 
See Figure~\ref{fig:vertex-cover}.  
Note that, the solver for the decision-version can be used to answer 
whether any graph, in this case the remaining graph $G_{-v}$, can be covered by $(k^* - 1)$ vertices. 

\begin{figure}[h]
\centering{\input{vertex-cover}}
\caption{Illustrating that a vertex $v$ is in one minimum vertex cover if and only if
$G_{-v}$ can be covered by $k^* - 1$ vertices, where $k^*$ is defined as the cardinality of any minimum vertex cover of $G$.}
%\emph{Proof.} If $G_{-v}$ can be covered by $k^* - 1$ vertices, i.e., there exists a vertex cover $V_2^*$
%for $G_{-v}$ with $|V_2^*| = k^* - 1$, then $V_2^* \cup \{v\}$ is a vertex cover of $G$,
%and its cardinality is $k^*$. This shows that that $V_2^* \cup \{v\}$ is a minimum vertex cover of $G$ 
%as its size is minimized, and it includes $v$. On the other hand, 
\label{fig:vertex-cover}
\end{figure}


\begin{minipage}{0.8\textwidth}
	\aaA {9}{algorithm-for-$VC(G)$}\xxx
	\aab {$k^* \leftarrow $ algorithm-for-cardinality-$VC(G)$;}\xxx
	\aaB {4}{for each $v \in V$}\xxx
	\aac {build the remaining graph $G_{-v}$ by removing $v$ and its adjacent edges from $G$;}\xxx
	\aac {call the solver for the decision-version to test $G_{-v}$ and $k^*-1$: $B \leftarrow $ solver-for-$VC(G_{-v}, k^*-1)$;}\xxx
	\aac {if $B$ = ``yes'': save this $v$ as $u$ and break the for-loop;}\xxx
	\aab {end for;}\xxx
	\aab {resolve the remaining graph $G_{-u}$: $V_2^* \leftarrow $ algorithm-for-$VC(G_{-u})$;}\xxx
	\aab {return $\{u\}\cup V_2^*$;}\xxx
	\aaa {end;}\xxx
\end{minipage}

The complete algorithm for the optimization version is given above.
Note that this algorithm is a recursive algorithm, and we define
that algorithm-for-$VC(G)$ returns the actual minimum vertex cover of $G$.
What's its running time? Note that it calls itself once, i.e., 
algorithm-for-$VC(G_{-u})$, and the number of vertices in this call
is reduced by 1. Besides, it calls algorithm-for-cardinality-$VC(G)$,
which requires $|V|$ calls to solver-for-$VC(G, k)$,
and the for-loop requires another $|V|$ calls to solver-for-$VC(G, k)$.
Combined, its running time $T(n)$, where $n = |V|$, can be written
as $T(n) = T(n-1) + 2n\cdot solver(G,k)$, where $solver(G,k)$ represents
the running time of calling solver-for-$VC(G, k)$ once.
Solving this recurrence gives $T(n) = O(n^2)\cdot solver(G,k)$.

This proves $VC(G) \le_p VC(G,k)$, as we designed an algorithm
for $VC(G)$, which uses $O(n^2)$, a polynomial number, of calls
to a solver for $VC(G, k)$. \qed

These two facts show that the decision-version and the optimization-version
of the vertex cover problem are essentially equivalent, in terms of that they are mutually polynomial-time reducible.
Such conclusion are true for most optimization problems.
This is useful in simplying the hierarchy of problems: now we can solely focus on decision problems, as they are simpler~(the output is just a single bit),
while also captures the hardness of the optimization version~(equivalent in terms of polynomial-time reduction).

\subsection*{P and NP}

In complexity theory, ``P'' is defined as the set of decision problems that are \emph{polynomial-time solvable}, 
and ``NP'' is defined as the set of decision problems that are \emph{polynomial-time verifiable}, 
Polynomial-time solvable means that one can design a polynomial-time algorithm to \emph{solve} the problem.
Polynomial-time verifiable means that one can design a polynomial-time algorithm to \emph{verify}
if a \emph{given} solution~(also called a \emph{certificate}) is correct,  
and such certificate must exist for ``yes''-instances.
They are formally defined below.

%formally stated below. 

\begin{definition}[P]
Let $X$ be a decision problem. We say $X\in \textrm{P}$ if there exists a polynomial-time algorithm $A$~(we call $A$ as a \emph{solver}), such that,\\
	(1), solver $A$ takes an instance $x$ of $X$ as input, and \\
	(2), for every instance $x$ of problem $X$: \\
	\hspace*{0.2cm} if the ground-truth of $x$ is ``yes'', then $A(x)$ returns ``yes'';\\
	\hspace*{0.2cm} if the ground-truth of $x$ is ``no'', then $A(x)$ returns ``no''.
\end{definition}

In above definition, condition (1) means that algorithm $A$ is \emph{for solving} problem $X$;
condition (2) means that algorithm $A$ is correct: every instance has a ``ground-truth'' answer, and
algorithm $A$ always finds it.

\begin{definition}[NP]
Let $X$ be a decision problem. We say $X\in \textrm{NP}$ if there exists a polynomial-time algorithm $A$~(we call $A$ as a \emph{verifier}), such that,\\
	(1), verifier $A$ takes an instance $x$ of $X$ \emph{and} a polynomial-length certificate $c$ as input, and \\
	(2), for every instance $x$ of problem $X$: \\
	\hspace*{0.2cm} if the ground-truth of $x$ is ``yes'', then there must exist a certificate $c$ such that $A(x,c)$ returns ``yes'';\\
	\hspace*{0.2cm} if the ground-truth of $x$ is ``no'', then for any polynomial-length certificate $c$, $A(x,c)$ always returns ``no''.
\end{definition}

In above definition, condition (1) means that algorithm $A$ is for verifying if the (given) certificate is the correct solution of the (given) instance;
condition (2) means there exists (polynomial-length) certificate
for every ``yes''-instnace but not for any ``no''-instance, and they can be correctly verified by algorithm $A$.
Notice that, \emph{how to find} such correct certificate is not the job of algorithm $A$.
When we use this definition to show that a problem is in NP, we just need to argue that
such certificate \emph{exists}, we don't need to design algorithms to find it out.
We give two examples below.

\begin{fact}
3SAT problem is in NP.
\end{fact}
\emph{Proof.} By definition, to prove 3SAT is in NP, we need to design an
algorithm~(i.e., the verifier $A$), and show that it satisfies the two
conditions. For 3SAT, the certificate is an assignment for all variables,
which can be represented as a binary vector. The verifier, given below, will check
if the given binary vector is in correct length, and if it satisfies all clauses,
and these can be done in polynomial-time.

\begin{minipage}{0.8\textwidth}
	\aaA {4}{verifier~$(x, c)$}\xxx
	\aab {verify that the length of $c$ equals to the number of variables in instance $x$: if not, return ``no'';}\xxx
	\aab {verify that every clause in instance $x$ is true when $x_i$ is assigned with $c[i]$: if not, return ``no'';}\xxx
	\aab {return ``yes'';}\xxx
	\aaa {end;}\xxx
\end{minipage}

Now we prove that above verifier satisfies the two conditions.
It clearly takes an instance and a certificate as input.
If $x$ is an ``yes''-instance, then it means $x$ is satisfiable, i.e.,
there exists an assignment that satisfies all clauses.
Clearly, once this assignment is passed to the verifier together with $x$,
the verifier will return ``yes''.
If $x$ is an ``no''-instance, then it means $x$ is not satisfiable, i.e.,
there does not exist an assignment that satisfies all clauses.
Hence, no matter which assignment is passed to the verifier with $x$,
the verifier always returns ``no''. Also, verifier $A$ runs in polynomial-time.
This proves that 3SAT is in NP.
\qed


\begin{fact}
The decision-version of vertex cover problem, i.e., $VC(G, k)$, is in NP.
\end{fact}

\emph{Proof.} Again we design an algorithm~(i.e., the verifier $A$), and show that it satisfies the two
conditions. For $VC(G, k)$, the certificate is a subset of vertices, which
can be represented as a vector.
The verifier, given below, will verify its size, and verify if it covers all edges.

\begin{minipage}{0.8\textwidth}
	\aaA {4}{verifier~$(x = (G, k), c)$}\xxx
	\aab {verify the size of $c$ is at most $k$: if not, return ``no'';}\xxx
	\aab {verify the vertices in $c$ can cover all edges of $G$: if not, return ``no'';}\xxx
	\aab {return ``yes'';}\xxx
	\aaa {end;}\xxx
\end{minipage}

Above verifier takes an instance and a certificate as input.
If $x = (G, k)$ is an ``yes''-instance, then it means there exists a subset of vertices $V_1$
with $|V_1| \le k$ and $V_1$ covers all edges in $G$.
Hence, once this $V_1$ is passed to the verifier together with $x$,
the verifier will return ``yes''.
If $x$ is an ``no''-instance, then it does not exist a subset with at most $k$ vertices
that covers all edges in $G$.
Hence, no matter which subset of vertices~(coded in $c$) is passed to the verifier with $x$,
the verifier always returns ``no''---either because it exceeds $k$ vertices,
or does not cover all edges.  Also, verifier $A$ runs in polynomial-time.
\qed


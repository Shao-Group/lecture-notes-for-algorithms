\setcounter{definition}{0} \setcounter{property}{0} \setcounter{claim}{0} \setcounter{fact}{0} \setcounter{corollary}{0} \setcounter{figure}{0}
\section{Longest Increasing Subsequences}

\subsection*{Framework of Dynamic Programming Algorithm}

A typical dynamic programming algorithm consists of following steps:
\vspace*{-\topsep}
\begin{enumerate}
\item Define subproblems~(or equivalently, partition the search space into sub-spaces),
$P_1, P_2, \cdots, P_n$; solving all these subproblems will solve the original problem.
\item Develop a recursion for these subproblems. Formally, let $opt(P_k)$ be the optimal
solution of $P_k$. A recursion is a function that calculates
$opt(P_{k+1})$ using $opt(P_1), opt(P_2), \cdots, opt(P_k)$.
\item Fill a \emph{dynamic programming table},
in which each entry stores the optimal solution of a subproblem, 
using above recursion.
\end{enumerate}

\subsection*{Optimal Substrcture Property}

Let $P_1, P_2, P_3, \cdots, P_n$ be a series of subproblems defined for original problem $P$.
If we have that, the optimal solution of $P_k$, denoted as $opt(P_k)$,
can be calculated solely from the \emph{optimal solution} of previous subproblems,
i.e., $opt(P_k) = f(opt(P_1), opt(P_2), \cdots, opt(P_{k-1}))$,
then we say problem $P$ satisfies the optimal substructure property.
Intuitively, the optimal substructure property states that
the optimal solution of a larger problem can be \emph{assembled}
using the optimal solutions of the smaller subproblems.

How to see if a problem satisfies the optimal substructure property?
Usually we can verify if its opporsite side is true, i.e., if the optimal solution
of a bigger subproblem \emph{implies} the optimal solution of
the smaller subproblems.
Take the shortest path problem as an example we showed before:
Let $p = s \to w \to \cdots \to u \to v$
be the shortest path from $s$ to $v$.
Then clearly its sub-path $s\to w \to \cdots \to u$ is the shortest path from
$s$ to $u$~(because, if there exists a shorter path from $s$ to $u$
then we can find a shorter one from $u$ to $v$ than $p$).
This suggests that the shortest path problem
satisfies the optimal substructure property.
We will use more examples to demonstrate it
and how to determine a proper definition accordingly.


\subsection*{Longest Increasing Subsequences}

Let $A[1\cdots n]$ be an array with $n$ distinct integers.
A \emph{subsequence} of array $A$ is a subset of $A$ but follows
the same order. For example, $(A[2], A[4], A[5])$ is a subsequence of $A$
while $(A[3], A[1], A[4])$ is not.
An \emph{increasing subsequence} of $A$ means the values of the
the subsequence are increasing.
Formally, an increasing subsequence of length $k$ can be written
as $(A[i_1], A[i_2], \cdots, A[i_k])$ where $1\le i_1 < i_2 < \cdots < i_k \le n$,
and $A[i_1] < A[i_2] < \cdots < A[i_k]$.
The \emph{longest increasing subsequence problem} is, given array $A[1\cdots n]$,
to seek an increasing sequence with largest length.

For example, let $A = [1, 5, 3, 8, 9, 4, 7, 6]$. Then one of the longest
increasing sequence is $[1, 3, 4, 7]$. Notice that, the optimal solution
may not be unique. In this example, you may find more increasing sequences 
of length 4.

We want to design a dynamic programming algorithm to solve this problem.
Recall that, in order to give a proper definition of subproblems,
we can try ``reverse engineering'', i.e., starting from the optimal
solution, trying to determine if it \emph{implies} the optimal solution
of certain subproblem. If it does, then that subproblem is likely the one
we could use in designing the DP algorithm.

Let's try this approach with above example. We know that $[1, 3, 4, 7]$
is one optimal solution. Does its substructure, $[1, 3, 4]$, give the optimal
solution of certain subproblem? A natural guess would be that, $[1, 3, 4]$,
is the longest increasing subsequence of $A[1\cdots 6] = [1, 5, 3, 8, 9, 4]$.
But, a further look denies this conjecture, as $[1, 3, 8, 9]$ is a longer
increasing subsequence of $A[1\cdots 6]$.
Let's have another try. We still consider $A[1\cdots 6]$, but require that
the last number of the increasing subsequence must be $A[6] = 4$.
(Note: this is again a commonly-used technique, i.e., introducing a variable or a constraint
into the definition of the subproblems.)
Among all such increasing subsequences of $A[1\cdots 6]$, is $[1, 3, 4]$ the optimal one?
Yes! Why? Again we can prove by contradiction: suppose that there is a longer increasing subsequence of $A[1\cdots 6]$
that ends at $A[6] = 4$, we can concatenate it with $A[7] = 7$ to obtain 
a longer, than $[1, 3, 4, 7]$, increasing subsequence of $A$, contradicting
to the optimality of $[1, 3, 4, 7]$.

We summarize above example with the following optimal substructure statement.
\begin{fact}\label{fact1}
If $(A[i_1], A[i_2], \cdots, A[i_k])$ be the longest increasing subsequence of $A[1\cdots i_k]$,
then among all increasing subsequences of $A[1\cdots i_{k-1}]$ with last number being $A[i_{k-1}]$,
$(A[i_1], A[i_2], \cdots, A[i_{k-1}])$ is (one of) the longest one. 
\end{fact}

Above property directly suggests the definition of subproblems.
We define $opt(k)$ as the longest increasing subsequence
of $A[1\cdots k]$ with $A[k]$ being the last number,
and define $L(k)$ as the length of $opt(k)$.
In other words, among all increasing subsequences of $A[1\cdots k]$ with last number being $A[k]$,
$opt(k)$ is the optimal solution and $L(k)$ is its length.

We defined $n$ subproblems. Suppose we can solve all of them.
What's the longest increasing subsequence of $A$?
Clearly, that's $\max_{1\le k \le n} L(k)$.

We now develop a recursion. 
How to calculate $opt(k)$?
Again we enumerate the last step. By definition of the subproblem, $A[k]$ is the last number of $opt(k)$.
We therefore enumerate its second last number, which must be one of $A[j]$ satisfying $1\le j < k$ and $A[j] < A[k]$.
We don't know which one is correct so we enumerate all possibilities.
(And once we enumerate, we can assume we know it.)
Suppose that $A[j]$ is the second last number of $opt(k)$, then according to Fact~\ref{fact1},
$opt(k)$ can be obtained by calculating $opt(j)$ followed by $A[k]$.
Formally, the recursion is:
$$\textstyle L(k) = 1 + \max_{1 \le j < k, A[j] < A[k]} L[j].$$

We give the pseudo-code below.
We maintain two arrays, $L(k)$, to store the length of the $opt(k)$,
and $P(k)$, to maintain the tracking-back pointers, $1 \le k \le n$.
The optimal solution $opt(k)$ can be constructed by using $T$.
The running time of the algorithm is $O(n^2)$.

\begin{minipage}{0.8\textwidth}
	\aaA {12}{Algorithm longest-increasing-sequence~(array $A[1\cdots n]$)}\xxx
	\aaB {9}{for $k = 1 \to n$}\xxx
	\aac {$L(k) = 1$;}\xxx
	\aac {$T(k) = 0$;}\xxx
	\aaC {5}{for $j = 1 \to k - 1$}\xxx
	\aaD {3}{if~($A[j] < A[k]$ and $L(j) + 1 > L(k)$)}\xxx
	\aae {$L(k) = L(j) + 1$;}\xxx
	\aae {$T(k) = j$;}\xxx
	\aad {end if;}\xxx
	\aac {end for;}\xxx
	\aab {end for;}\xxx
	\aab {report: $\max_{1\le k \le n} L(k)$ is the length of the longest increasing subsequence of $A$;}\xxx
	\aaa {end algorithm;}\xxx
\end{minipage}

